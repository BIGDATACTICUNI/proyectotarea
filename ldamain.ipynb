{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24909\n",
      "                                       headline_text  index\n",
      "0  Analista Funcional Senior TI Adexus Peru Lima ...      0\n",
      "1  Practicante Supervision Bancaria Dpto c Superi...      1\n",
      "2  Analista Programador Java Semi Senior TI Adexu...      2\n",
      "3  Analista Programador Java Junior TI Adexus Per...      3\n",
      "4  Analista Programador .Net Senior TI Adexus Per...      4\n"
     ]
    }
   ],
   "source": [
    "# Importamos data de titulos de noticias de los últimos 15 años obtenidos de Kaggle\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv', encoding = \"latin-1\", error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "\n",
    "#Agrega una columna adicional como index basado en los indexes de la misma estructura\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "\n",
    "#Ejecutaremos los siguientes pasos:\n",
    "\n",
    "# 1. Tokenization: Parte el texto en sentencias y las sentencias en palabras. Las palabras se ponen en minuscula\n",
    "#                  y se remueve la puntuación\n",
    "# 2. Palabras que tienen menos de 3 caracteres son removidos.\n",
    "# 3. Se eliminan todas las palabras de parada\n",
    "# 4. Las palabras son lematizadas: Las palabras en tercera persona son cambiadas a primera persona \n",
    "#                                  y los verbos en pasado y futuro son cambiado a presente\n",
    "# 5. Las palabras se derivan: Las palabras se reducen en su forma raiz\n",
    "\n",
    "# Loading gensim and nltk libraries\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se escribe una funcion para ejecutar la lematización y preprocesamiento en el conjunto de datos\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Si', 'estás', 'en', 'el', 'I', 'Seminario', 'de', 'Buenas', 'Prácticas', 'Laborales,', 'recuerda', 'que', 'utilizando', 'el', 'hashtag', '#SeminarioAptitus', 'pued\\x85', 'https://t.co/wf22t8Jdwf']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['está', 'seminario', 'buena', 'práctica', 'laboral', 'recuerda', 'utilizando', 'hashtag', 'pu', 'http', 'jdwf']\n"
     ]
    }
   ],
   "source": [
    "# seleccionamos un documento para visualizar luego del preprocesamiento\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "doc_sample = documents[documents['index'] == 10310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [analista, funcion, senior, adexu, peru, lima,...\n",
      "1    [practicant, supervis, bancaria, dpto, banca, ...\n",
      "2    [analista, programador, java, semi, senior, ad...\n",
      "3    [analista, programador, java, junior, adexu, p...\n",
      "4    [analista, programador, senior, adexu, peru, l...\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]\n",
    "\n",
    "print(processed_docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 adexu\n",
      "1 analista\n",
      "2 funcion\n",
      "3 http\n",
      "4 lima\n",
      "5 peru\n",
      "6 senior\n",
      "7 yryujjmi\n",
      "8 banca\n",
      "9 bancaria\n",
      "10 dpto\n",
      "11 practicant\n",
      "12 qslyklox\n",
      "13 supervis\n",
      "14 java\n",
      "15 programador\n",
      "16 semi\n",
      "17 zrim\n",
      "18 junior\n",
      "19 mfej\n",
      "20 uprm\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words on the Data set\n",
    "# Crear un diccionaro basado en 'processed_docs' que contiene \n",
    "# el número de veces que una palabra aparece en los datos de entrenamiento\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim filter_extremes\n",
    "\n",
    "#  Filtra los tokens que aparecen en\n",
    "# 1. menos de 15 documentos (número absoluto) o \n",
    "# 2. más que 0.5 documentos (fracción del tamaño del cuerpo total, no es número absoluto)\n",
    "# 3. Después de los dos pasos anteriores, mantenga solo los primeros 100000 tokens más frecuentes.\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (339, 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim doc2bow\n",
    "# Para cada documento creamos un diccionario que reporte cuantas palabras\n",
    "# and cuantas veces estas palabras aparecen. Guarde esto en 'bow_corpus',\n",
    "# entonces checkee el documento seleccionado previamente.\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[1310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"analista\") appears 1 time.\n",
      "Word 339 (\"trabajando\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Previsualizar Bag of Words para nuestra muestra documentos procesados\n",
    "bow_doc_1310 = bow_corpus[1310]\n",
    "for i in range(len(bow_doc_1310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1310[i][0], \n",
    "                                               dictionary[bow_doc_1310[i][0]], \n",
    "bow_doc_1310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.34707701501073235),\n",
      " (1, 0.5948847080332527),\n",
      " (2, 0.26954550707210595),\n",
      " (3, 0.3934193540003902),\n",
      " (4, 0.5460917151125703)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# Crear el objeto de modelo tf-idf usando models.TfidModel en 'bow_corpus'\n",
    "# y guardarlo en 'tdidf', luego aplicar tranformación al cuerpo entero y\n",
    "# llamarlo 'corpus_tfidf'. Finalmente previsualizamos los scores TF-IDF\n",
    "# para nuestro primer documento\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using Bag of Words\n",
    "# Entrenar nuestro modelo lda usando gensim.models.LdaMulticore y \n",
    "# guardarlo en 'lda_model'\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.079*\"overal\" + 0.041*\"perú\" + 0.035*\"está\" + 0.030*\"lima\" + 0.028*\"mbtska\" + 0.027*\"empresa\" + 0.027*\"transna\" + 0.026*\"buscando\" + 0.024*\"empleo\" + 0.023*\"búsqueda\"\n",
      "Topic: 1 \n",
      "Words: 0.060*\"nuestra\" + 0.046*\"atención\" + 0.044*\"oportunidad\" + 0.038*\"postula\" + 0.037*\"esta\" + 0.033*\"para\" + 0.031*\"asistent\" + 0.029*\"mbtska\" + 0.029*\"ingresa\" + 0.023*\"client\"\n",
      "Topic: 2 \n",
      "Words: 0.064*\"para\" + 0.031*\"empleo\" + 0.029*\"laboral\" + 0.025*\"trabajo\" + 0.021*\"oferta\" + 0.019*\"mejor\" + 0.019*\"tien\" + 0.017*\"week\" + 0.016*\"labor\" + 0.015*\"overal\"\n",
      "Topic: 3 \n",
      "Words: 0.158*\"overal\" + 0.063*\"corporativo\" + 0.052*\"servicio\" + 0.041*\"ejecutivo\" + 0.039*\"desarrollo\" + 0.028*\"impulsamo\" + 0.025*\"busi\" + 0.023*\"impulsadora\" + 0.018*\"mbtska\" + 0.017*\"vshyfm\"\n",
      "Topic: 4 \n",
      "Words: 0.054*\"ingresa\" + 0.049*\"búsqueda\" + 0.048*\"encontramo\" + 0.047*\"mbtska\" + 0.034*\"asistent\" + 0.032*\"practicant\" + 0.024*\"profesion\" + 0.020*\"lima\" + 0.019*\"nuestro\" + 0.019*\"portal\"\n",
      "Topic: 5 \n",
      "Words: 0.072*\"para\" + 0.043*\"teleoperador\" + 0.019*\"empresa\" + 0.018*\"portaltrabajo\" + 0.016*\"montacarguista\" + 0.015*\"equipo\" + 0.014*\"person\" + 0.013*\"lima\" + 0.013*\"atención\" + 0.012*\"calidad\"\n",
      "Topic: 6 \n",
      "Words: 0.058*\"empresa\" + 0.042*\"venta\" + 0.030*\"promotor\" + 0.027*\"portaltrabajo\" + 0.027*\"nuestro\" + 0.026*\"client\" + 0.022*\"servicio\" + 0.019*\"comerci\" + 0.015*\"jefe\" + 0.015*\"técnico\"\n",
      "Topic: 7 \n",
      "Words: 0.064*\"empresa\" + 0.049*\"empleo\" + 0.039*\"perú\" + 0.035*\"trabajo\" + 0.030*\"entrevista\" + 0.027*\"perfil\" + 0.026*\"completo\" + 0.025*\"líder\" + 0.023*\"transna\" + 0.023*\"venta\"\n",
      "Topic: 8 \n",
      "Words: 0.034*\"para\" + 0.018*\"overal\" + 0.015*\"pued\" + 0.014*\"trabajo\" + 0.014*\"ingresa\" + 0.013*\"puesto\" + 0.013*\"semana\" + 0.013*\"labor\" + 0.012*\"familia\" + 0.011*\"est\"\n",
      "Topic: 9 \n",
      "Words: 0.102*\"perú\" + 0.074*\"empleo\" + 0.073*\"empresa\" + 0.056*\"transna\" + 0.038*\"ingresa\" + 0.030*\"mbtska\" + 0.029*\"postula\" + 0.028*\"búsqueda\" + 0.021*\"para\" + 0.018*\"busca\"\n"
     ]
    }
   ],
   "source": [
    "# Para cada tópico, exploraremos las palabras que aparecen en ese tópico y su peso relativo.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "# Puedes distinguir los diferentes tópicos usando las palabras en cada tópico y sus pesos correspondientes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.020*\"perú\" + 0.019*\"portaltrabajo\" + 0.015*\"empleo\" + 0.014*\"transna\" + 0.013*\"para\" + 0.013*\"empresa\" + 0.010*\"lima\" + 0.009*\"búsqueda\" + 0.009*\"postula\" + 0.008*\"limpieza\"\n",
      "Topic: 1 Word: 0.018*\"overal\" + 0.016*\"para\" + 0.016*\"corporativo\" + 0.016*\"oportunidad\" + 0.013*\"comienza\" + 0.013*\"promocion\" + 0.013*\"área\" + 0.013*\"trabajo\" + 0.012*\"portaltrabajo\" + 0.012*\"administrativa\"\n",
      "Topic: 2 Word: 0.020*\"perú\" + 0.018*\"portaltrabajo\" + 0.018*\"asesor\" + 0.016*\"comerci\" + 0.015*\"experiencia\" + 0.015*\"empleo\" + 0.014*\"transna\" + 0.013*\"empresa\" + 0.011*\"teleoperador\" + 0.011*\"impulsamo\"\n",
      "Topic: 3 Word: 0.014*\"humano\" + 0.012*\"facebook\" + 0.011*\"para\" + 0.011*\"nuestra\" + 0.010*\"analista\" + 0.010*\"overal\" + 0.009*\"trabajo\" + 0.009*\"recurso\" + 0.008*\"profesion\" + 0.007*\"oscar\"\n",
      "Topic: 4 Word: 0.028*\"portal\" + 0.026*\"completo\" + 0.025*\"perfil\" + 0.023*\"postular\" + 0.021*\"pued\" + 0.017*\"vshyfm\" + 0.015*\"overal\" + 0.015*\"ingresando\" + 0.014*\"visita\" + 0.013*\"empleo\"\n",
      "Topic: 5 Word: 0.022*\"week\" + 0.016*\"overal\" + 0.015*\"éxito\" + 0.012*\"follow\" + 0.012*\"familia\" + 0.011*\"asistent\" + 0.011*\"perú\" + 0.011*\"practicant\" + 0.010*\"lleva\" + 0.009*\"empresa\"\n",
      "Topic: 6 Word: 0.023*\"perú\" + 0.017*\"empleo\" + 0.016*\"para\" + 0.015*\"empresa\" + 0.014*\"transna\" + 0.010*\"overal\" + 0.009*\"trabajo\" + 0.008*\"reclutamiento\" + 0.008*\"industri\" + 0.008*\"tien\"\n",
      "Topic: 7 Word: 0.019*\"perú\" + 0.018*\"twitter\" + 0.013*\"overal\" + 0.012*\"empleo\" + 0.011*\"desarrollo\" + 0.011*\"transna\" + 0.011*\"empresa\" + 0.010*\"impulsamo\" + 0.010*\"cajero\" + 0.009*\"correo\"\n",
      "Topic: 8 Word: 0.063*\"perú\" + 0.040*\"transna\" + 0.036*\"empleo\" + 0.035*\"empresa\" + 0.020*\"lima\" + 0.016*\"semana\" + 0.015*\"venta\" + 0.015*\"buen\" + 0.011*\"vendedor\" + 0.010*\"inicio\"\n",
      "Topic: 9 Word: 0.016*\"perú\" + 0.016*\"empresa\" + 0.016*\"client\" + 0.016*\"programador\" + 0.015*\"nuestro\" + 0.014*\"jefe\" + 0.012*\"empleo\" + 0.012*\"venta\" + 0.011*\"servicio\" + 0.010*\"analista\"\n"
     ]
    }
   ],
   "source": [
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    \n",
    "# Nuevamente, puedes distinguir los diferentes tópicos usando las palabras en cada tópico y sus pesos correspondientes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analista',\n",
       " 'programador',\n",
       " 'java',\n",
       " 'semi',\n",
       " 'senior',\n",
       " 'adexu',\n",
       " 'peru',\n",
       " 'lima',\n",
       " 'http',\n",
       " 'zrim']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del desempeño clasificando un documento de muestra usando \n",
    "# el modelo LDA Bag of Words\n",
    "# Checkeare donde nuestro documento de texto sería clasificado\n",
    "\n",
    "processed_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8714066743850708\t \n",
      "Topic: 0.064*\"empresa\" + 0.049*\"empleo\" + 0.039*\"perú\" + 0.035*\"trabajo\" + 0.030*\"entrevista\" + 0.027*\"perfil\" + 0.026*\"completo\" + 0.025*\"líder\" + 0.023*\"transna\" + 0.023*\"venta\"\n",
      "\n",
      "Score: 0.01429153885692358\t \n",
      "Topic: 0.060*\"nuestra\" + 0.046*\"atención\" + 0.044*\"oportunidad\" + 0.038*\"postula\" + 0.037*\"esta\" + 0.033*\"para\" + 0.031*\"asistent\" + 0.029*\"mbtska\" + 0.029*\"ingresa\" + 0.023*\"client\"\n",
      "\n",
      "Score: 0.01429047528654337\t \n",
      "Topic: 0.054*\"ingresa\" + 0.049*\"búsqueda\" + 0.048*\"encontramo\" + 0.047*\"mbtska\" + 0.034*\"asistent\" + 0.032*\"practicant\" + 0.024*\"profesion\" + 0.020*\"lima\" + 0.019*\"nuestro\" + 0.019*\"portal\"\n",
      "\n",
      "Score: 0.01428951881825924\t \n",
      "Topic: 0.158*\"overal\" + 0.063*\"corporativo\" + 0.052*\"servicio\" + 0.041*\"ejecutivo\" + 0.039*\"desarrollo\" + 0.028*\"impulsamo\" + 0.025*\"busi\" + 0.023*\"impulsadora\" + 0.018*\"mbtska\" + 0.017*\"vshyfm\"\n",
      "\n",
      "Score: 0.014288264326751232\t \n",
      "Topic: 0.079*\"overal\" + 0.041*\"perú\" + 0.035*\"está\" + 0.030*\"lima\" + 0.028*\"mbtska\" + 0.027*\"empresa\" + 0.027*\"transna\" + 0.026*\"buscando\" + 0.024*\"empleo\" + 0.023*\"búsqueda\"\n",
      "\n",
      "Score: 0.01428766455501318\t \n",
      "Topic: 0.072*\"para\" + 0.043*\"teleoperador\" + 0.019*\"empresa\" + 0.018*\"portaltrabajo\" + 0.016*\"montacarguista\" + 0.015*\"equipo\" + 0.014*\"person\" + 0.013*\"lima\" + 0.013*\"atención\" + 0.012*\"calidad\"\n",
      "\n",
      "Score: 0.014286941848695278\t \n",
      "Topic: 0.058*\"empresa\" + 0.042*\"venta\" + 0.030*\"promotor\" + 0.027*\"portaltrabajo\" + 0.027*\"nuestro\" + 0.026*\"client\" + 0.022*\"servicio\" + 0.019*\"comerci\" + 0.015*\"jefe\" + 0.015*\"técnico\"\n",
      "\n",
      "Score: 0.014286569319665432\t \n",
      "Topic: 0.102*\"perú\" + 0.074*\"empleo\" + 0.073*\"empresa\" + 0.056*\"transna\" + 0.038*\"ingresa\" + 0.030*\"mbtska\" + 0.029*\"postula\" + 0.028*\"búsqueda\" + 0.021*\"para\" + 0.018*\"busca\"\n",
      "\n",
      "Score: 0.014286546036601067\t \n",
      "Topic: 0.064*\"para\" + 0.031*\"empleo\" + 0.029*\"laboral\" + 0.025*\"trabajo\" + 0.021*\"oferta\" + 0.019*\"mejor\" + 0.019*\"tien\" + 0.017*\"week\" + 0.016*\"labor\" + 0.015*\"overal\"\n",
      "\n",
      "Score: 0.014285771176218987\t \n",
      "Topic: 0.034*\"para\" + 0.018*\"overal\" + 0.015*\"pued\" + 0.014*\"trabajo\" + 0.014*\"ingresa\" + 0.013*\"puesto\" + 0.013*\"semana\" + 0.013*\"labor\" + 0.012*\"familia\" + 0.011*\"est\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    \n",
    "# Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que \n",
    "# nuestro modelo asignó, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8714082837104797\t \n",
      "Topic: 0.016*\"perú\" + 0.016*\"empresa\" + 0.016*\"client\" + 0.016*\"programador\" + 0.015*\"nuestro\" + 0.014*\"jefe\" + 0.012*\"empleo\" + 0.012*\"venta\" + 0.011*\"servicio\" + 0.010*\"analista\"\n",
      "\n",
      "Score: 0.014290133491158485\t \n",
      "Topic: 0.063*\"perú\" + 0.040*\"transna\" + 0.036*\"empleo\" + 0.035*\"empresa\" + 0.020*\"lima\" + 0.016*\"semana\" + 0.015*\"venta\" + 0.015*\"buen\" + 0.011*\"vendedor\" + 0.010*\"inicio\"\n",
      "\n",
      "Score: 0.014288833364844322\t \n",
      "Topic: 0.014*\"humano\" + 0.012*\"facebook\" + 0.011*\"para\" + 0.011*\"nuestra\" + 0.010*\"analista\" + 0.010*\"overal\" + 0.009*\"trabajo\" + 0.009*\"recurso\" + 0.008*\"profesion\" + 0.007*\"oscar\"\n",
      "\n",
      "Score: 0.014288203790783882\t \n",
      "Topic: 0.020*\"perú\" + 0.019*\"portaltrabajo\" + 0.015*\"empleo\" + 0.014*\"transna\" + 0.013*\"para\" + 0.013*\"empresa\" + 0.010*\"lima\" + 0.009*\"búsqueda\" + 0.009*\"postula\" + 0.008*\"limpieza\"\n",
      "\n",
      "Score: 0.014287931844592094\t \n",
      "Topic: 0.028*\"portal\" + 0.026*\"completo\" + 0.025*\"perfil\" + 0.023*\"postular\" + 0.021*\"pued\" + 0.017*\"vshyfm\" + 0.015*\"overal\" + 0.015*\"ingresando\" + 0.014*\"visita\" + 0.013*\"empleo\"\n",
      "\n",
      "Score: 0.014287693426012993\t \n",
      "Topic: 0.019*\"perú\" + 0.018*\"twitter\" + 0.013*\"overal\" + 0.012*\"empleo\" + 0.011*\"desarrollo\" + 0.011*\"transna\" + 0.011*\"empresa\" + 0.010*\"impulsamo\" + 0.010*\"cajero\" + 0.009*\"correo\"\n",
      "\n",
      "Score: 0.014287518337368965\t \n",
      "Topic: 0.018*\"overal\" + 0.016*\"para\" + 0.016*\"corporativo\" + 0.016*\"oportunidad\" + 0.013*\"comienza\" + 0.013*\"promocion\" + 0.013*\"área\" + 0.013*\"trabajo\" + 0.012*\"portaltrabajo\" + 0.012*\"administrativa\"\n",
      "\n",
      "Score: 0.014287499710917473\t \n",
      "Topic: 0.020*\"perú\" + 0.018*\"portaltrabajo\" + 0.018*\"asesor\" + 0.016*\"comerci\" + 0.015*\"experiencia\" + 0.015*\"empleo\" + 0.014*\"transna\" + 0.013*\"empresa\" + 0.011*\"teleoperador\" + 0.011*\"impulsamo\"\n",
      "\n",
      "Score: 0.014286966063082218\t \n",
      "Topic: 0.023*\"perú\" + 0.017*\"empleo\" + 0.016*\"para\" + 0.015*\"empresa\" + 0.014*\"transna\" + 0.010*\"overal\" + 0.009*\"trabajo\" + 0.008*\"reclutamiento\" + 0.008*\"industri\" + 0.008*\"tien\"\n",
      "\n",
      "Score: 0.014286964200437069\t \n",
      "Topic: 0.022*\"week\" + 0.016*\"overal\" + 0.015*\"éxito\" + 0.012*\"follow\" + 0.012*\"familia\" + 0.011*\"asistent\" + 0.011*\"perú\" + 0.011*\"practicant\" + 0.010*\"lleva\" + 0.009*\"empresa\"\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del desempeño clasificando un documento de muestra utilizando el modelo LDA TF-IDF.\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "    \n",
    "# Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que nuestro \n",
    "# modelo asignó, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3666684627532959\t Topic: 0.064*\"para\" + 0.031*\"empleo\" + 0.029*\"laboral\" + 0.025*\"trabajo\" + 0.021*\"oferta\"\n",
      "Score: 0.3666570484638214\t Topic: 0.034*\"para\" + 0.018*\"overal\" + 0.015*\"pued\" + 0.014*\"trabajo\" + 0.014*\"ingresa\"\n",
      "Score: 0.03334025293588638\t Topic: 0.158*\"overal\" + 0.063*\"corporativo\" + 0.052*\"servicio\" + 0.041*\"ejecutivo\" + 0.039*\"desarrollo\"\n",
      "Score: 0.03333413600921631\t Topic: 0.072*\"para\" + 0.043*\"teleoperador\" + 0.019*\"empresa\" + 0.018*\"portaltrabajo\" + 0.016*\"montacarguista\"\n",
      "Score: 0.03333338722586632\t Topic: 0.079*\"overal\" + 0.041*\"perú\" + 0.035*\"está\" + 0.030*\"lima\" + 0.028*\"mbtska\"\n",
      "Score: 0.03333333507180214\t Topic: 0.060*\"nuestra\" + 0.046*\"atención\" + 0.044*\"oportunidad\" + 0.038*\"postula\" + 0.037*\"esta\"\n",
      "Score: 0.03333333507180214\t Topic: 0.054*\"ingresa\" + 0.049*\"búsqueda\" + 0.048*\"encontramo\" + 0.047*\"mbtska\" + 0.034*\"asistent\"\n",
      "Score: 0.03333333507180214\t Topic: 0.058*\"empresa\" + 0.042*\"venta\" + 0.030*\"promotor\" + 0.027*\"portaltrabajo\" + 0.027*\"nuestro\"\n",
      "Score: 0.03333333507180214\t Topic: 0.064*\"empresa\" + 0.049*\"empleo\" + 0.039*\"perú\" + 0.035*\"trabajo\" + 0.030*\"entrevista\"\n",
      "Score: 0.03333333507180214\t Topic: 0.102*\"perú\" + 0.074*\"empleo\" + 0.073*\"empresa\" + 0.056*\"transna\" + 0.038*\"ingresa\"\n"
     ]
    }
   ],
   "source": [
    "# Testing model on unseen document\n",
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
